{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex10_computation_and_cognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuvUmU0iOuL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VwrmZqxd7gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/C&C/ex10_files/python\")\n",
        "import utils, Merchant_00000001, Merchant_00000002, Merchant_00000003"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKbbX0UFZzRO",
        "colab_type": "text"
      },
      "source": [
        "###Q2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO-PPQ9dZ51g",
        "colab_type": "text"
      },
      "source": [
        "####2.1\n",
        "The average reward for Merchant_1 vs. Merchant_2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSoQQZS9Pd5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9af384e-fd08-4473-9d87-4ed2bd0105d4"
      },
      "source": [
        "utils.run_game(\"Merchant_00000001\", \"Merchant_00000002\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.602, 6.564)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDHZd8hka7DD",
        "colab_type": "text"
      },
      "source": [
        "####2.2\n",
        "Strategies\\\n",
        "\n",
        "*   Merchant_1:\\\n",
        "In the first game the player chooses randomly his action (with probability 0.5 to each action). Afterwards, the player chooses the opposite action from his last game.\\\n",
        "\n",
        "*   Merchant_2:\\\n",
        "The player chooses randomly his action (with probability 0.5 to each action) without considering his history and his opponent history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY85x_PweT69",
        "colab_type": "text"
      },
      "source": [
        "####2.3\n",
        "My Algorithm\\\n",
        "There is no \"pure strategy\", but there is a \"best response\": if the opponent chooses 'L' we better choose H, and if the opponent chooses 'H' we better choose 'L. Therfore, we will try to predict the  next action of the opponent, and we will choose the opposite action.\\\n",
        "The prediction of the opponent's next action is based on the previous sequence of our choices and the opponent's response to this sequence. We will check our previous $i$ choices ($i$ ranges from 1 to $\\min${10, number of games played}. We won't check sequences larger then 10 for reducing complexity and avoiding 'overfitting' to the opponent older choiches).\\\n",
        "Then, we will see what was the responses by the opponent to this sequence in the history, and will compute the mean of the responses (we will sign it by $res(i)=$ the mean response for last-sequence($i$)).\\\n",
        "We assumes that the opponent gives more importance to our recent choices and less importance to the older choices (long sequences). Therfore, we will assign weights to each mean-response, depending on what sequence it came from: $w=(\\frac{1}{2})^i$ (approx. summed up to 1). Then, each $i$ will get a score equals to $w(i)*res(i)$. Note that this scoring method requires us to represent 'L' action by -1 instead of 0.\\\n",
        "Finally, we will take the simple mean of the scores (which are ranged from -1 to 1) and compute its sign, for 1 we will return 0 and for -1 we will return 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuVv5FoAeVNj",
        "colab_type": "text"
      },
      "source": [
        "2.4\\\n",
        "Attached is a .py file"
      ]
    }
  ]
}